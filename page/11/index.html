<html>
  <head>
	<meta name="generator" content="Hugo 0.54.0" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155190626-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-155190626-1');
    </script>
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css"></link>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/skeleton/2.0.4/skeleton.css"></link>
    <link href="https://fonts.googleapis.com/css?family=Lato|M+PLUS+1p|M+PLUS+Rounded+1c|Roboto&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/css/base.css"></link>
    
  <link rel="stylesheet" type="text/css" href="/css/index.css"></link>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body class="container">
    <header>
      <h1><a href="/">Coda</a></h1>
      <nav>
	<ul>
	  <li><a href="/posts/">Posts</a></li>
	  <li><a href="/about/">About</a></li>
	</ul>
      </nav>
    </header>
    <main>
      
  
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/are_we_really_making_much_progress/">メモ Are We Really Making Much Progress? A Worring Analysis of Recent Neural Recomendation Approaches</a></h2>
	  <p>August 10, 2019</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>表題は、ニューラルネットワークを用いた推薦システムを提案、評価した論文における実験の再現性と予測性能の再評価した論文のタイトルにあたる。
発表学会は、2019年のRecSys。著者らは、以下の2つのRQに回答するためにトップ会議で発表された18の論文を調査した。その結果、実験を再現できた論文は7稿であり、その中でも単純な手法を上回る性能が認められたのは1稿だけだった。</p>

<ul>
<li>RQ1: ニューラルネットワークを用いた推薦システムの研究の再現性はどの程度か</li>
<li>RQ2: 最近発表されたアルゴリズムは、ハイパーパラメタチューニングされた単純な手法と比べてどの程度性能がいいか</li>
</ul></div>
	<a class="readmore" href="/posts/are_we_really_making_much_progress/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/gaussian_processes_for_regression/">メモ Gaussian Processes for Regression</a></h2>
	  <p>August 3, 2019</p>
	</header>
	<div class="excerpt"><p>表題はガウス過程の回帰問題への応用を提案した論文。著者らは、scikit-learnの<a href="https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process">ガウス過程回帰</a>の
元になっている<a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">Gaussian Processes for Machine Learning</a>の著者と同じ。
論文の構成は、ガウス過程回帰の予測分布の式、ハイパーパラメタ推定方法、実験による評価からなる。</p></div>
	<a class="readmore" href="/posts/gaussian_processes_for_regression/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/google_vizier/">概要 Google Vizier: A Service for Black-Box Optimization</a></h2>
	  <p>August 3, 2019</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>表題にあるVizierはGoogleにおいてデファクトになっているブラックボックス最適化のためのサービスであり、
論文は、Vizierのシステムアーキテクチャの構成とアルゴリズムの説明とその評価からなる。</p></div>
	<a class="readmore" href="/posts/google_vizier/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/learning_active_learning_from_data/">概要 Learning Active Learning from Data</a></h2>
	  <p>July 27, 2019</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>表題にある論文は、次にラベルを与えるべきデータが何かという能動学習における問題を、
あるサンプルを教師データに追加したときの損失関数の減少値を予測する回帰の問題としてとらえる。
能動学習の目的は最小限データで最大の予測性能をもつモデルを構築することであり、次にアノテーションすべきデータが何かを正しく予測することが課題になる。
論文は、アノテーションすべきサンプルを予測する回帰モデルを学習するアルゴリズムを提案、評価する。アルゴリズムは2値分類の分類器を対象としている。</p></div>
	<a class="readmore" href="/posts/learning_active_learning_from_data/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/textrank/">概要 TextRank: Bringing Order into Texts</a></h2>
	  <p>July 20, 2019</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>表題にある論文は、ドキュメントからキーワードとキーセンテンスを抽出するためのグラフベースのアルゴリズムTextRankを提案、評価した。
TextRankは、名前から推測できるようにPageRankを応用した手法であり、頂点の重要度を、頂点の内容のような局所的な情報ではなく、他の頂点との辺の接続関係を含むグラフ全体の大域的な情報から決定する。PageRankとTextRankのアルゴリズムの違いは、TextRankの場合は辺ごとに重みが設定できるところにある。</p></div>
	<a class="readmore" href="/posts/textrank/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/subword_regularization/">概要 Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</a></h2>
	  <p>July 17, 2019</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>表題は、過去に<a href="../neural_machine_translation_of_rate_words/">紹介</a>した<a href="https://www.aclweb.org/anthology/P16-1162">論文</a>と同様、sentencepiece（ニューラルネットワークを用いた言語処理向けのトークナイザ・脱トークナイザ）の元になった論文にあたる。
ノイズに対する頑強さのために、単語のサブワード（部分文字列）を生成するユニグラム言語モデルの学習方法と、モデルから生成されたサブワード列を入力とする機械翻訳モデルの学習方法を提案した。</p></div>
	<a class="readmore" href="/posts/subword_regularization/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/neural_machine_translation_of_rate_words/">概要 Neural Machine Trasnslation of Rare Words with Subword Units</a></h2>
	  <p>July 13, 2019</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>内容は、sentencepiece（ニューラルネットワークを用いた言語処理向けのトークナイザ・脱トークナイザ）のトークナイズで使われるアルゴリズムになっている。単語をサブワード（単語の部分文字列）に分割し、サブワードを組み合わせて珍しい単語や未知語を表現することで、これらの出現頻度の低い単語の翻訳上げるというもの。</p></div>
	<a class="readmore" href="/posts/neural_machine_translation_of_rate_words/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/embedding_logical_queries_on_knowledge_graphs/">メモ Embedding Logical Queries on Knowledge Graphs</a></h2>
	  <p>February 17, 2019</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>一階述語論理式で表現されたクエリを満たすノードを、分散表現に変換し、ナレッジグラフの中から計算時間上効率よく見つけるアルゴリズムを提案した。
クエリに現れるエッジの数に対して計算時間が線形であることが特徴。
ただし、クエリには、存在量化と連接を使えるが、全称量化、選択、否定を使うことができない制約がある。</p></div>
	<a class="readmore" href="/posts/embedding_logical_queries_on_knowledge_graphs/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/regularizing_and_optimizing_lstm_language_models/">Regularizing and Optimizing LSTM Language Models</a></h2>
	  <p>November 23, 2018</p>
	</header>
	<div class="excerpt"><p>本稿は、LSTMを用いた言語モデルに対して正規化と最適化を適用し、実験を通して既存の先行研究とperplexityの観点で予測性能を評価した。本稿の手法の利点は、LSTMの実装に変更を加えずに適用できるために、NVIDIA cuDNNなどの高速でブラックボックスなライブラリで実装できることにある。</p></div>
	<a class="readmore" href="/posts/regularizing_and_optimizing_lstm_language_models/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/deep_joint_entity_disambiguation/">メモ Deep Joint Entity Disambiguation with Local Neural Attention</a></h2>
	  <p>November 9, 2018</p>
	</header>
	<div class="excerpt"><p>本稿は、当ページで紹介した<a href="https://aclweb.org/anthology/K18-1050">End-to-End Neural Entity Linking</a>(End-to-End) の著者らの先行研究にあたる。
End-to-EndがEntity LinkingのMention Detection(MD)とEntity Disambiguation(ED)の両方をアプローチの対象にしているのに対し、本稿ではEDのみが対象となっている。
したがって、文章からmention（参照表現）が抽出されていることが前提にあり、提案の中心は、参照表現に対応するエンティティを候補の中から正しく求める手法にある。</p></div>
	<a class="readmore" href="/posts/deep_joint_entity_disambiguation/">Read more</a>
      </article>
    
  
  <div class="pagination">
  
    <a href="/"><i class="material-icons">first_page</i></a>
  
  
    <a href="/page/10/"><i class="material-icons">chevron_left</i></a>
  
  
    <a href="/page/12/"><i class="material-icons">chevron_right</i></a>
  
  
    <a href="/page/13/"><i class="material-icons">last_page</i></a>
  
</div>


    </main>
    <footer>
      <ul>
       
       <li>
	 <a href="https://github.com/nryotaro"><i class="fab fa-github"></i></a>
       </li>
       
       
       <li>
	 <a href="https://www.linkedin.com/in/nakamura-ryotaro">
	   <i class="fab fa-linkedin"></i>
	 </a>
       
       </li>
	<li><a href="/index.xml"><i class="fas fa-rss"></i></a></li>
      </ul>
      <small>&copy; Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>
</html>
