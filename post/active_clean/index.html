<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    
    <title>
      
      メモ ActiveClean: Interactive Data Cleaning For Statistical Modeling - Coda
      
		</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link href="https://fonts.googleapis.com/css?family=M+PLUS+Rounded+1c" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/assets/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/normalize.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/icons.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/screen.css" />
    
    <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Lato:100,100i,300,300i,400,400i,700,700i|Source+Code+Pro:300,400,500,700" rel="stylesheet">
    

    
    <script src="https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/bigfoot/dist/bigfoot.js"></script>
    <link rel="stylesheet" type="text/css" href="/assets/bigfoot/dist/bigfoot-number.css" />
    <script type="text/javascript">
        $.bigfoot();
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    
    
</head>

    <body class="post-template">
        <header class="main-header">
	<div class="main-header-content">
		<h1 class="blog-title">
			<a href="/">
				
           Coda
				
			</a>
		</h1>
		<h2 class="blog-description"></h2>
	</div>

	<div class="nav">
    
		
	</div>
</header>

        
<main class="content" role="main">
  <article class="post">
    <header class="post-header">
      
      <h2 class="post-title">メモ ActiveClean: Interactive Data Cleaning For Statistical Modeling</h2>
      <section class="post-meta">
        <time class="post-date">November 09, 2019</time>
      </section>
    </header>
    <section class="post-content">
      <p>ActiveCleanは、教師データの誤りを修正し、モデルの精度を改善する手法である。
優先して修正すべきデータを推定し、データが修正されたら修正後のデータでモデルを学習し、再び推定する工程を反復する。
反復的な学習により大域的最適解に到達できるモデルであれば、ActiveCleanの推定は最適解への収束を保証される。
データの修正件数が等しい条件のもとで、先行研究と比べて最大2.5倍の精度改善を達成した。</p>

<p>ActiveCleanの全体像を以下の図に示す。外側の実線がActiveCleanの範囲であり、六角形はモデルを示す。
<img src="/active_clean/architecture.png" alt="Architecture" />
循環する矢印が示す通り、ActiveCleanにより推定されたデータをユーザが修正し、修正されたデータでモデルのパタメタを更新する、この繰返しがActiveCleanの手順である。
大域的最適解への収束と汚れたデータの選出の最適化は、モデルの学習に使われるSGDの性質によって、保証されている。</p>

<p>通常のSGDによる学習との違いは、汚れのある教師データで、修正後の損失関数\(\phi\)の最小値を推定することにある。
以下の図はパラメタの更新の様子を表す。縦軸と横軸は、それぞれ損失関数とパラメタの値に対応する。
赤線と青線は、汚れの有無が異なる損失関数のグラフである。
<img src="/active_clean/fig3.png" alt="fig3" />
左図が示すように、汚れありのデータで求まる最適なパラメタと汚れなしのデータが異なる。
そこで、これまでに修正したデータをもとに、汚れのないデータで求める損失関数の勾配を推定し、パラメタを更新する。
パラメタを\(\theta\)とすると、勾配の推定値\(g(\theta)\)が次のように定められている。</p>

<p>$$
g_C (\theta) = \frac{\mid R_\mathcal{clean}\mid}{\mid R\mid}\cdot g_{C}(\theta)+\frac{\mid R_\mathcal{dirty}\mid}{\mid R\mid}\cdot g_S(\theta)
$$</p>

<p>$$
g_C (\theta) = \frac{1}{\mid R_\mathcal{clean}\mid}\sum_{i\in R_\mathcal{clean}} \nabla\phi(x_i^{( c )}, y_i^{( c )}, \theta)
$$</p>

<p>$$
g_S (\theta) = \frac{1}{\mid S\mid}\sum_{i\in S}\frac{1}{p(i)}\nabla\phi(x_i^{( c )}, y_i^{( c )}, \theta)
$$</p>

<p>\((x_i^{( c )}, y_i^{( c )})\)は修正後のデータとラベルの組、\(R\)は\(x\)のと1対1の関係にあるレコード\(r\)の集合であり、修正とはレコード\(r\)を\(r&rsquo;\)ないしは\(\emptyset\)に変換する操作をいう。
また、\(S\)はバッチである。
\(R_\mathcal{clean}\)は修正後のレコードの集合, \(R_\mathcal{dirty}\)は確率\(p\)で選ばれていないレコードの集合を示す。</p>

<p>\(t\)回目の繰返しにおいては、学習率を\(\gamma\)としてパラメタは以下の式で更新される。
$$
\theta^{(t+1)} \leftarrow \theta^{(t)} - \gamma\cdot g(\theta^{(t)})
$$</p>

<p>今度は、修正すべきレコードの選びかた\(p( r )\)を定める。
バッチサイズと反復回数が定数の場合、\(g^*\)を汚れのない訓練データで計算した勾配と仮定すると、収束率は、\(\mathcal{O}(\sigma)\)が上界になる。ただし、\(\sigma\)を</p>

<p>$$
\sigma^{2} = \mathbb{E}(\mid\mid g- g^{*}\mid\mid^{2})
$$</p>

<p>とする。そこで</p>

<p>$$
    \underset{p}{\operatorname{argmin}}\ \mathbb{E}(\mid\mid g_S - g^* \mid\mid^{2})
$$
が成り立つように\(p\)を定める。
<a href="http://tongzhang-ml.org/papers/icml15-sois.pdf">[1]</a>により
\(p_i \propto\mid\mid\nabla\phi(x_i^{( c )}, y_i^{( c )}, \theta^{(t)})\mid\mid\)であり、
また、パラメタの更新と同様に、汚れのないデータを現在のデータの状態から推定するという考えで、</p>

<p>$$
\begin{aligned}
\nabla\phi(x_i^{( c )}, y_i^{( c )}, \theta^{(t)}) &amp;\approx \nabla\phi(x_i^{(d)}, y_i^{(d)}, \theta^{(t)})\\<br />
&amp; + \frac{\partial}{\partial X}\nabla\phi(x_i^{(d)}, y_i^{(d)}, \theta^{(t)})\cdot (x^{(d)} - x^{( c)}) \\<br />
&amp; + \frac{\partial}{\partial Y}\nabla\phi(x_i^{(d)}, y_i^{(d)}, \theta^{(t)})\cdot (x^{(d)} - x^{( c)})
\end{aligned}
$$</p>

<!--
$$
\nabla\phi(x\_i^{( c )}, y\_i^{( c )}, \theta^{(t)}) \approx \phi (x\_i^{(d)}, y\_i^{(d)}, \theta^{(t)})+M\_x\cdot\Delta x+M\_y \cdot \Delta y
$$
-->

<p>と近似することで\(p(i)\)を求める。\((x_i^{(d)}, y_i^{(d)\)は修正前のデータを示す。
<!-- \\(M\_x\\)と\\(M\_y\\)はモデルの種類に依存する行列であり、\-->
論文では、大域的最適解が保証されるモデルをconvex loss modelsとよんでいる。
convex loss modelsにあたるモデルには、SVMや線形回帰、ロジスティック回帰などがある。
Convex loss modelsでないモデルの場合、データを修正する前のモデルのパラメタに最も近い局所最適解に収束するので、最終的な精度はモデルの初期状態に依存する。</p>

<hr />

<p>論文は<a href="http://www.vldb.org/pvldb/vol9/p948-krishnan.pdf">こちら</a>からダウンロードできます。
文章にある論文はすべて論文から引用されています。</p>
    </section>
    <footer class="post-footer">
      
    </footer>
  </article>
</main>

        <footer class="site-footer">
  <section class="rss"><a class="subscribe-button icon-feed" href="/index.xml"></a></section>
  
  
  <section>
    <a class="fas fa-rss" href="/index.xml"></a>
    <a class="fab fa-github" href="https://github.com/nryotaro"></a>
    <a class="fab fa-linkedin" href="https://www.linkedin.com/in/nakamura-ryotaro"></a>
  </section>
  <section class="copyright">&copy; 2019 Nakamura, Ryotaro</section>
</footer>



    </body>
</html>
