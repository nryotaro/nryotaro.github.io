<html>
  <head>
	<meta name="generator" content="Hugo 0.66.0" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155190626-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-155190626-1');
    </script>
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css"></link>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/skeleton/2.0.4/skeleton.css"></link>
    <link href="https://fonts.googleapis.com/css?family=Lato|M+PLUS+1p|M+PLUS+Rounded+1c|Roboto&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/css/base.css"></link>
    
  <link rel="stylesheet" type="text/css" href="/css/index.css"></link>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body class="container">
    <header>
      <h1><a href="/">Coda</a></h1>
      <nav>
	<ul>
	  <li><a href="/posts/">Posts</a></li>
	  <li><a href="/about/">About</a></li>
	</ul>
      </nav>
    </header>
    <main>
      
  
  
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/the_seven_sins/">論文メモ The Seven Sins: Security Smells in Infrastructure as Code Scripts</a></h2>
	  <p>March 20, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>
<p>OSSの調査にもとづき、Infrastrucure as Code(IaC)スクリプトに潜む主要なセキュリティ上の不吉な匂い(Security Smells)を7つ列挙し、これらを検出するツールを実装した論文である。
論文のねらいは、開発者がIaCスクリプトに不吉な匂いを混ぜないようにすることにある。
著者らは、本論文で、ICSE2019のDistinguished Paper Awardを受賞した。</p></div>
	<a class="readmore" href="/posts/the_seven_sins/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/semi_supervised_sequence_learning/">論文メモ Semi-supervised Sequence Learning</a></h2>
	  <p>March 14, 2020</p>
	</header>
	<div class="excerpt"><p>系列データの教師あり学習において、ラベルのないデータを学習した言語モデルやオートエンコーダーの重みでLSTMを初期化することの有用性を実験的に示した。</p></div>
	<a class="readmore" href="/posts/semi_supervised_sequence_learning/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/raft/">論文メモ In Search of an Understandable Consensus Algorithm</a></h2>
	  <p>March 9, 2020</p>
	</header>
	<div class="excerpt"><p>Raftとよばれるコンセンサスアルゴリズムを提案した論文である。
Raftは、Multi Paxosと同様の実行結果をもたらす。
実行するコマンドのログをサーバ間で交換することで、状態を同期し、サーバの一部が落ちてもシステムを継続することができる。</p></div>
	<a class="readmore" href="/posts/raft/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/sentence_piece/">論文メモ SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></h2>
	  <p>February 29, 2020</p>
	</header>
	<div class="excerpt"><p>SentencePieceは、深層学習向けのトークナイザ・脱トークナイザである。
特定の言語を意識した処理がないため、あらゆるテキストに利用できる。
本論文では、C++やPythonによる<a href="https://github.com/google/sentencepiece">実装</a>と翻訳への適用実験について書かれている。
アルゴリズムの解説は、<a href="https://www.aclweb.org/anthology/P16-1162.pdf">Sennrich et al.</a>や<a href="https://arxiv.org/pdf/1804.10959.pdf">Kudo.</a>にゆずられている。
これらの論文について2019年7月13日の<a href="../neural_machine_translation_of_rate_words/">記事</a>と2019年7月17日の<a href="./subword_regularization/">記事</a>で解説している。</p></div>
	<a class="readmore" href="/posts/sentence_piece/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/latent_dirichlet_allocation/">論文メモ Latent Dirichlet Allocation</a></h2>
	  <p>February 23, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>
<p>トピックモデルの潜在的ディリクレ配分法(LDA)の原論文である。
LDAは、テキストコーパスのような離散データの確率的生成モデルである。
意味のあるデータのまとまりに対する端的な説明を与える情報を見つけることを目的としている。
3つの階層からなる階層ベイズモデルである。
、データの要素は、各トピックを表すモデルの混合モデルから生成される。
トピックもまた混合モデルから確率的に生成される。
推論にはベイズ変分法を、パラメタの推定にはEMアルゴリズムをもちいらる。</p></div>
	<a class="readmore" href="/posts/latent_dirichlet_allocation/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/unsupervised_pretraining_for_sequence_to_sequence_learning/">論文メモ Unsupervised Pretraining for Sequence to Sequence Learning</a></h2>
	  <p>February 16, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>
<p>事前学習とファインチューニングにより<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">seq2seq</a>の汎化性能を改善する手法を提案した論文である。
encoderの重みを学習済み言語モデルの重みで初期化する。
decoderについても、encoderと別の言語モデルを用意し、その重みで初期化する。
ただし、工夫のないファインチューニングをすると<a href="https://arxiv.org/pdf/1312.6211.pdf">破滅的忘却</a>が生じてしまう。
そこで、ファインチューニングでは言語モデルとseq2seqの目的関数の両方を学習につかうことで、過学習をさけ、汎化性能を確保する。</p></div>
	<a class="readmore" href="/posts/unsupervised_pretraining_for_sequence_to_sequence_learning/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/playing_atari_with_deep_reinforcement_learning/">論文メモ Playing Atari with Deep Reinforcement Learning</a></h2>
	  <p>February 9, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>
<p>深層強化学習をAtari2600の7つのゲームに応用し、うち6つについて先行手法の性能を超えたDeep Q-Networks(DQN)を提案した論文である。
ピクセルデータを直接入力として与え、深層学習で方策を学習する手法としては初めて提案された。</p></div>
	<a class="readmore" href="/posts/playing_atari_with_deep_reinforcement_learning/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/context2vec/">論文メモ context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a></h2>
	  <p>February 2, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>
<p>文書の文脈の分散表現を獲得するニューラルネットワークのアーキテクチャ<em>context2vec</em>を提案、評価した論文である。
アーキテクチャの基本構造は<a href="https://arxiv.org/pdf/1301.3781.pdf">CBOW</a>と同様で、周辺の単語から中心の単語を当てられるようにコーパスをもとにモデルを訓練する。
CBOWとの違いは、文脈の算出方法にある。
CBOWは、ウィンドウ内のベクトルの平均値で文脈の分散表現を求める。
一方、<em>context2vec</em>では、双方向LSTMの出力をもとに算出する。</p>
<!-- raw HTML omitted --></div>
	<a class="readmore" href="/posts/context2vec/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/neural_machine_translation_by_jointly_learning_to_align_and_translate/">論文メモ NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></h2>
	  <p>February 1, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>
<p>Decoderに注意機構を採用したencoder-decoderモデルを提案した論文である。
ICLR2015で発表された。
論文の発表当時、encoder-decoderモデルによる翻訳の多くは、encoderが入力文を固定長ベクトルに変換し、固定長ベクトルから翻訳された文を出力していた。
著者らは、固定長ベクトルへの変換が長い文の翻訳性能を下げていると考え、固定長ベクトルを注意機構におきかえたencoder-decoderモデルを提案する。
モデルは、翻訳に加え、生成する単語と入力文の箇所の関係を学習する。
推定時には、まず、次に生成する単語に関係する入力文の箇所を推定する。
次に、推定された箇所と生成済の単語列をもとに、単語を生成する。
特に長い文書の翻訳において、固定長ベクトルをつかうモデルよりも、提案手法が優れていることを実験的に示した。</p></div>
	<a class="readmore" href="/posts/neural_machine_translation_by_jointly_learning_to_align_and_translate/">Read more</a>
      </article>
    
  
  <div class="pagination">
  
    <a><i class="material-icons">first_page</i></a>
  
  
    <a><i class="material-icons">chevron_left</i></a>
  
  
    <a href="/page/2/"><i class="material-icons">chevron_right</i></a>
  
  
    <a href="/page/7/"><i class="material-icons">last_page</i></a>
  
</div>


    </main>
    <footer>
      <ul>
       
       <li>
	 <a href="https://github.com/nryotaro"><i class="fab fa-github"></i></a>
       </li>
       
       
       <li>
	 <a href="https://www.linkedin.com/in/nakamura-ryotaro">
	   <i class="fab fa-linkedin"></i>
	 </a>
       
       </li>
	<li><a href="/index.xml"><i class="fas fa-rss"></i></a></li>
      </ul>
      <small>&copy; Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>
</html>
